{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvyH9iMH1vPw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from itertools import combinations\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def L2discrepancy(x):\n",
        "    N = x.size(1)\n",
        "    dim = x.size(2)\n",
        "    prod1 = 1. - x ** 2.\n",
        "    prod1 = torch.prod(prod1, dim=2)\n",
        "    sum1 = torch.sum(prod1, dim=1)\n",
        "    pairwise_max = torch.maximum(x[:, :, None, :], x[:, None, :, :])\n",
        "    product = torch.prod(1 - pairwise_max, dim=3)\n",
        "    sum2 = torch.sum(product, dim=(1, 2))\n",
        "    one_dive_N = 1. / N\n",
        "    out = torch.sqrt(math.pow(3., -dim) - one_dive_N * math.pow(2., 1. - dim) * sum1 + 1. / math.pow(N, 2.) * sum2)\n",
        "    return out\n",
        "\n",
        "def hickernell_all_emphasized(x,dim_emphasize):\n",
        "    nbatch, nsamples, dim = x.size(0), x.size(1), x.size(2)\n",
        "    mean_disc_projections = torch.zeros(nbatch).to(device)\n",
        "    for d in dim_emphasize:\n",
        "        subsets_of_d = list(combinations(range(dim), d))\n",
        "        for i in range(len(subsets_of_d)):\n",
        "            set_inds = subsets_of_d[i]\n",
        "            mean_disc_projections += L2discrepancy(x[:, :, set_inds])\n",
        "\n",
        "    return mean_disc_projections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xyjh37gkAdZ",
        "outputId": "93dea8e7-fb77-4f00-ae12-9a96df26c0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6u8HDc_3hkP",
        "outputId": "37a791a0-9c28-40dc-8abd-cf2f6570e9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=3622730 sha256=b0d556722d956772278b18db8c484b1bd055f1875f352a57e38f302cf98ba459\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp311-cp311-linux_x86_64.whl size=2846216 sha256=cc2fd746ba8d4ceed57e1eefaeff3f771b81753b779a96bbe33fdc6c6cf79091\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/e2/1e/299c596063839303657c211f587f05591891cc6cf126d94d21\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Collecting torch-cluster\n",
            "  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (2.0.2)\n",
            "Building wheels for collected packages: torch-cluster\n",
            "  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp311-cp311-linux_x86_64.whl size=2057703 sha256=874825efb44d7ba3f5a1525e5adbcd37bfa54246e55a21a21b3d8e0c7ddb2536\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/de/7d/a4211822af99147b93800e9e204f0be21294e3c0b95b3b861a\n",
            "Successfully built torch-cluster\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading torch_spline_conv-1.2.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch-spline-conv\n",
            "  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-spline-conv: filename=torch_spline_conv-1.2.2-cp311-cp311-linux_x86_64.whl size=566631 sha256=555bb6bbc73e5ee5ef106083edc1382aa6e3dc07af677b69e9c5c4ccdabef9bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/16/8a/a98b0173c4fbbc7aa1c4929b46d2eb08d1475c5c7b54e289b6\n",
            "Successfully built torch-spline-conv\n",
            "Installing collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from torch import nn\n",
        "from torch_cluster import radius_graph\n",
        "from torch_geometric.nn import MessagePassing, InstanceNorm\n",
        "\n",
        "class MPNN_layer(MessagePassing):\n",
        "    def __init__(self, ninp, nhid):\n",
        "        super(MPNN_layer, self).__init__()\n",
        "        self.ninp = ninp\n",
        "        self.nhid = nhid\n",
        "\n",
        "        self.message_net_1 = nn.Sequential(nn.Linear(2 * ninp, nhid),\n",
        "                                           nn.ReLU()\n",
        "                                           )\n",
        "        self.message_net_2 = nn.Sequential(nn.Linear(nhid, nhid),\n",
        "                                           nn.ReLU()\n",
        "                                           )\n",
        "        self.update_net_1 = nn.Sequential(nn.Linear(ninp + nhid, nhid),\n",
        "                                          nn.ReLU()\n",
        "                                          )\n",
        "        self.update_net_2 = nn.Sequential(nn.Linear(nhid, nhid),\n",
        "                                          nn.ReLU()\n",
        "                                          )\n",
        "        self.norm = InstanceNorm(nhid)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.propagate(edge_index, x=x)\n",
        "        x = self.norm(x, batch)\n",
        "        return x\n",
        "\n",
        "    def message(self, x_i, x_j):\n",
        "        message = self.message_net_1(torch.cat((x_i, x_j), dim=-1))\n",
        "        message = self.message_net_2(message)\n",
        "        return message\n",
        "\n",
        "    def update(self, message, x):\n",
        "        update = self.update_net_1(torch.cat((x, message), dim=-1))\n",
        "        update = self.update_net_2(update)\n",
        "        return update\n",
        "\n",
        "\n",
        "class MPMC_net(nn.Module):\n",
        "    def __init__(self, dim, nhid, nlayers, nsamples, nbatch, radius, loss_fn, dim_emphasize, n_projections):\n",
        "        super(MPMC_net, self).__init__()\n",
        "        self.enc = nn.Linear(dim,nhid)\n",
        "        self.convs = nn.ModuleList()\n",
        "        for i in range(nlayers):\n",
        "            self.convs.append(MPNN_layer(nhid,nhid))\n",
        "        self.dec = nn.Linear(nhid,dim)\n",
        "        self.nlayers = nlayers\n",
        "        self.mse = torch.nn.MSELoss()\n",
        "        self.nbatch = nbatch\n",
        "        self.nsamples = nsamples\n",
        "        self.dim = dim\n",
        "        self.n_projections = n_projections\n",
        "        self.dim_emphasize = torch.tensor(dim_emphasize).long()\n",
        "\n",
        "        ## random input points for transformation:\n",
        "        self.x = torch.rand(nsamples * nbatch, dim).to(device)\n",
        "        batch = torch.arange(nbatch).unsqueeze(-1).to(device)\n",
        "        batch = batch.repeat(1, nsamples).flatten()\n",
        "        self.batch = batch\n",
        "        self.edge_index = radius_graph(self.x, r=radius, loop=True, batch=batch).to(device)\n",
        "\n",
        "        if loss_fn == 'L2':\n",
        "            self.loss_fn = self.L2discrepancy\n",
        "        elif loss_fn == 'approx_hickernell':\n",
        "            if dim_emphasize != None:\n",
        "                assert torch.max(self.dim_emphasize) <= dim\n",
        "                self.loss_fn = self.approx_hickernell\n",
        "        else:\n",
        "            print('Loss function not implemented')\n",
        "\n",
        "    def approx_hickernell(self, X):\n",
        "        X = X.view(self.nbatch, self.nsamples, self.dim)\n",
        "        disc_projections = torch.zeros(self.nbatch).to(device)\n",
        "\n",
        "        for i in range(self.n_projections):\n",
        "            ## sample among non-emphasized dimensionality\n",
        "            mask = torch.ones(self.dim, dtype=bool)\n",
        "            mask[self.dim_emphasize - 1] = False\n",
        "            remaining_dims = torch.arange(1, self.dim + 1)[mask]\n",
        "            projection_dim = remaining_dims[torch.randint(low=0, high=remaining_dims.size(0), size=(1,))].item()\n",
        "            projection_indices = torch.randperm(self.dim)[:projection_dim]\n",
        "            disc_projections += self.L2discrepancy(X[:, :, projection_indices])\n",
        "            ## sample among emphasized dimensionality\n",
        "            remaining_dims = torch.arange(1, self.dim + 1)[self.dim_emphasize - 1]\n",
        "            projection_dim = remaining_dims[torch.randint(low=0, high=remaining_dims.size(0), size=(1,))].item()\n",
        "            projection_indices = torch.randperm(self.dim)[:projection_dim]\n",
        "            disc_projections += self.L2discrepancy(X[:, :, projection_indices])\n",
        "\n",
        "        return disc_projections\n",
        "\n",
        "    def L2discrepancy(self, x):\n",
        "        N = x.size(1)\n",
        "        dim = x.size(2)\n",
        "        prod1 = 1. - x ** 2.\n",
        "        prod1 = torch.prod(prod1, dim=2)\n",
        "        sum1 = torch.sum(prod1, dim=1)\n",
        "        pairwise_max = torch.maximum(x[:, :, None, :], x[:, None, :, :])\n",
        "        product = torch.prod(1 - pairwise_max, dim=3)\n",
        "        sum2 = torch.sum(product, dim=(1, 2))\n",
        "        one_dive_N = 1. / N\n",
        "        out = torch.sqrt(math.pow(3., -dim) - one_dive_N * math.pow(2., 1. - dim) * sum1 + 1. / math.pow(N, 2.) * sum2)\n",
        "        return out\n",
        "\n",
        "    def forward(self):\n",
        "        X = self.x\n",
        "        edge_index = self.edge_index\n",
        "\n",
        "        X = self.enc(X)\n",
        "        for i in range(self.nlayers):\n",
        "            X = self.convs[i](X,edge_index,self.batch)\n",
        "        X = torch.sigmoid(self.dec(X))  ## clamping with sigmoid needed so that warnock's formula is well-defined\n",
        "        X = X.view(self.nbatch, self.nsamples, self.dim)\n",
        "        loss = torch.mean(self.loss_fn(X))\n",
        "        return loss, X"
      ],
      "metadata": {
        "id": "S4IRYMVH3EPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from models import *\n",
        "#import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "#from utils import L2discrepancy, hickernell_all_emphasized\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def train(args):\n",
        "    model = MPMC_net(args.dim, args.nhid, args.nlayers, args.nsamples, args.nbatch,\n",
        "                     args.radius, args.loss_fn, args.dim_emphasize, args.n_projections).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    best_loss = 10000.\n",
        "    patience = 0\n",
        "\n",
        "    ## could be tuned for better performance\n",
        "    start_reduce = 100000\n",
        "    reduce_point = 10\n",
        "\n",
        "    Path('results/dim_' + str(args.dim)).mkdir(parents=True, exist_ok=True)\n",
        "    Path('outputs/dim_' + str(args.dim)).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        loss, X = model()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 100 ==0:\n",
        "            y = X.clone()\n",
        "            if args.loss_fn == 'L2':\n",
        "                batched_discrepancies = L2discrepancy(y.detach())\n",
        "            elif args.loss_fn == 'approx_hickernell':\n",
        "                ## compute sum over all projections with emphasized dimensionality:\n",
        "                batched_discrepancies = hickernell_all_emphasized(y.detach(),args.dim_emphasize)\n",
        "            else:\n",
        "                print('Loss function not implemented')\n",
        "            min_discrepancy, mean_discrepancy = torch.min(batched_discrepancies).item(), torch.mean(batched_discrepancies).item\n",
        "\n",
        "            if min_discrepancy < best_loss:\n",
        "                best_loss = min_discrepancy\n",
        "                f = open('results/dim_'+str(args.dim)+'/nsamples_'+str(args.nsamples)+'.txt', 'a')\n",
        "                f.write(str(best_loss) + '\\n')\n",
        "                f.close()\n",
        "\n",
        "                ## save MPMC points:\n",
        "                PATH = 'outputs/dim_'+str(args.dim)+'/nsamples_'+str(args.nsamples)+'.npy'\n",
        "                y = y.detach().cpu().numpy()\n",
        "                np.save(PATH,y)\n",
        "\n",
        "            if (min_discrepancy > best_loss and (epoch + 1) >= start_reduce):\n",
        "                patience += 1\n",
        "\n",
        "            if (epoch + 1) >= start_reduce and patience == reduce_point:\n",
        "                patience = 0\n",
        "                args.lr /= 10.\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = args.lr\n",
        "\n",
        "            if (args.lr < 1e-6):\n",
        "                f = open('results/dim_'+str(args.dim)+'/nsamples_'+str(args.nsamples)+'.txt', 'a')\n",
        "                f.write('### epochs: '+str(epoch) + '\\n')\n",
        "                f.close()\n",
        "                break\n",
        "\n",
        "\n",
        "class Customargs:\n",
        "  def __init__(self, dim =2, nhid = 128, nlayers = 3, nsamples = 64, nbatch = 16, radius = 0.35, loss_fn = 'L2', dim_emphasize = [1], n_projections = 15,\n",
        "               lr = 0.001, start_reduce = 100000, epochs = 200000, weight_decay = 1e-6):\n",
        "\n",
        "    self.dim = dim\n",
        "    self.nhid = nhid\n",
        "    self.nlayers = nlayers\n",
        "    self.nsamples = nsamples\n",
        "    self.nbatch = nbatch\n",
        "    self.radius = radius\n",
        "    self.loss_fn = loss_fn\n",
        "    self.dim_emphasize = dim_emphasize\n",
        "    self.n_projections = n_projections\n",
        "    self.lr = lr\n",
        "    self.epochs = epochs\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "\n",
        "\n",
        "args = Customargs(nsamples = 32)\n",
        "\n",
        "train(args)\n",
        "#args = parser.parse_args()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xB3Lna_8CG7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args8 = Customargs(nsamples = 96)\n",
        "train(args8)"
      ],
      "metadata": {
        "id": "U-GOOdBM2HdF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}